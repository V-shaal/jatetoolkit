= Introduction =
This guide introduces how to use JATE to:
  * extract terms from corpus using pre-implemented algorithms. See [JATEIntro#Extracting_terms_using_JATE Extracting terms using Jate].
  * extend JATE by implementing your own algorithms. See [JATEIntro#Implementing_new_algorithms Implementing new algorithms].

JATE (Java Automatic Term Extraction) is a toolkit for developing and experimenting Automatic Term Extractions/Recognition algorithms in Java. The motivation of this toolkit is to make available several state-of-the-art algorithms of ATE/ATR to developers and users of ATE/ATR, and encourage developers of ATE/ATR methods to develop their methods under a uniform framework to enable comparative studies.

JATE is implemented based on the common ground of most ATE/ATR algorithms, which typically follows the steps as below:

  * Extracting candidate terms from a corpus using linguistic tools
  * Extracting statistical features of candidates from the corpus
  * Apply ATE/ATR algorithms to score the domain representativeness of candidate terms based on their statistical features. The scores give an indication of the likelihood of a candidate term being a good domain specific term.

Following this, JATE can be divided into three main components:
  * Classes and utilities for extracting candidate terms
  * Classes for representing and building candidates’ features
  * Classes implementing state-of-the-art algorithms using the features

Currently JATEv1.1 has implemented 8 algorithms listed below:
  * Simple term frequency – the score of a candidate term is simply the frequency of that candidate noted in the corpus
  * TF.IDF – the score of a candidate term is the TF.IDF score of that candidate in the corpus
  * Weirdness – see Ahmad et a.l, 1999, Surrey Participation in TREC8: Weirdness Indexing for Logical Document Extrapolation
  * C-value – see Frantzi et al., 2000, Automatic recognition of multi-word terms:. the C-value/NC-value method
  * GlossEx – see Kozakov, et al., 2004, Glossary extraction and utilization in the information search and delivery system for IBM Technical Support
  * TermEx – see Sclano et al., 2007, TermExtractor: a Web application to learn the shared terminology of emergent web communities
  * RIDF - Residual IDF, see Church, K. and Gale, W. 1995a. Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. In Proceedings of the 3rd Workshop on Very Large Corpora. Cambridge, Massachusetts, USA, pp.121-30.
  * Average Term Frequency in Corpus - calculated by dividing the total frequency of a term in a corpus by its document frequency.

*Please note that we do not claim to have replicated each ATE/ATR method described in the reference*. We have made every effort to replicate the computation algorithm; however, you should expect to obtain slightly different results since we may have used different linguistic tools, and reference resources used by some algorithms. 


= Extracting terms using JATE =

The "main" method in the testing class "uk.ac.shef.dcs.oak.jate.test.AlgorithmTester" contains detailed explanations of how to use existing JATE algorithms for ATE/ATR. The code of this method is pasted below:
{{{
public static void main(String[] args) {
		if (args.length < 3) System.out.println("Usage: java AlgorithmTester [corpus_path] [reference_corpus_path] [output_folder]");
		else {
			try {
				System.out.println(new Date());

                //##########################################################
                //#         Step 1. Extract candidate terms/words from     #
                //#         documents, and index the terms/words, docs     #
                //#         and their relations (occur-in, containing)     #
                //##########################################################

                //stop words and lemmatizer are used for processing the extraction of candidate terms
				StopList stop = new StopList(true);
				Lemmatizer lemmatizer = new Lemmatizer();

                //Three CandidateTermExtractor are implemented:
                //1. An OpenNLP noun phrase extractor that extracts noun phrases as candidate terms
				//CandidateTermExtractor npextractor = new NounPhraseExtractorOpenNLP(stop, lemmatizer);
                //2. A generic N-gram extractor that extracts n(default is 5, see the property file) grams
                CandidateTermExtractor npextractor = new NGramExtractor(stop, lemmatizer);
                //3. A word extractor that extracts single words as candidate terms.
                //CandidateTermExtractor wordextractor = new WordExtractor(stop, lemmatizer);

                //This instance of WordExtractor is needed to build word frequency data, which are required by some algorithms
				CandidateTermExtractor wordextractor = new WordExtractor(stop, lemmatizer, false, 1);

                GlobalIndexBuilderMem builder = new GlobalIndexBuilderMem();
				GlobalIndexMem wordDocIndex = builder.build(new CorpusImpl(args[0]), wordextractor);
				GlobalIndexMem termDocIndex = builder.build(new CorpusImpl(args[0]), npextractor);

                //Optionally, you can save the index data as HSQL databases on file system
               // GlobalIndexWriterHSQL.persist(wordDocIndex, "D:/work/JATR_SDK/jate_googlecode/test/output/worddb");
               // GlobalIndexWriterHSQL.persist(termDocIndex, "D:/work/JATR_SDK/jate_googlecode/test/output/termdb");


                //##########################################################
                //#         Step 2. Build various statistical features     #
                //#         used by term extraction algorithms. This will  #
                //#         need the indexes built above, and counting the #
                //#         frequencies of terms                           #
                //##########################################################

                //A WordCounter instance is required to count number of words in corpora/documents
                WordCounter wordcounter = new WordCounter();

                //Next we need to count frequencies of candidate terms. This is a computational extensive process
                // and can take long for large corpus.
                //
                // There are two ways of doing this, the first is to use multi-thread
                // counting process, by which the corpus is split into sections and several threads are run in parallel
                // for counting; the second is a single thread option. The first will use more memory and CPU but faster
                // on large corpus.

                /* #1 Due to use of multi-threading, this can significantly occupy your CPU and memory resources. It is
                 better to use this way on dedicated server machines, and only for very large corpus
                * */
                FeatureCorpusTermFrequency wordFreq =
                        new FeatureBuilderCorpusTermFrequencyMultiThread(wordcounter, lemmatizer).build(wordDocIndex);
				FeatureDocumentTermFrequency termDocFreq =
                        new FeatureBuilderDocumentTermFrequencyMultiThread(wordcounter, lemmatizer).build(termDocIndex);
				FeatureTermNest termNest =
                        new FeatureBuilderTermNestMultiThread().build(termDocIndex);
				FeatureRefCorpusTermFrequency bncRef =
						new FeatureBuilderRefCorpusTermFrequency(args[1]).build(null);
				FeatureCorpusTermFrequency termCorpusFreq =
                        new FeatureBuilderCorpusTermFrequencyMultiThread(wordcounter, lemmatizer).build(termDocIndex);

                /* #2 */
                /*
                TermFreqCounter npcounter = new TermFreqCounter();
                FeatureCorpusTermFrequency wordFreq =
						new FeatureBuilderCorpusTermFrequency(npcounter, wordcounter, lemmatizer).build(wordDocIndex);
				FeatureDocumentTermFrequency termDocFreq =
						new FeatureBuilderDocumentTermFrequency(npcounter, wordcounter, lemmatizer).build(termDocIndex);
				FeatureTermNest termNest =
						new FeatureBuilderTermNest().build(termDocIndex);
				FeatureRefCorpusTermFrequency bncRef =
						new FeatureBuilderRefCorpusTermFrequency(args[1]).build(null);
				FeatureCorpusTermFrequency termCorpusFreq =
						new FeatureBuilderCorpusTermFrequency(npcounter, wordcounter, lemmatizer).build(termDocIndex);
                */


                //##########################################################
                //#         Step 3. For each algorithm you want to test    #
                //#         create an instance of the algorithm class,     #
                //#         and also an instance of its feature wrapper.   #
                //##########################################################
				AlgorithmTester tester = new AlgorithmTester();
				tester.registerAlgorithm(new TFIDFAlgorithm(), new TFIDFFeatureWrapper(termCorpusFreq));
				tester.registerAlgorithm(new GlossExAlgorithm(), new GlossExFeatureWrapper(termCorpusFreq, wordFreq, bncRef));
				tester.registerAlgorithm(new WeirdnessAlgorithm(), new WeirdnessFeatureWrapper(wordFreq, termCorpusFreq, bncRef));
				tester.registerAlgorithm(new CValueAlgorithm(), new CValueFeatureWrapper(termCorpusFreq, termNest));
				tester.registerAlgorithm(new TermExAlgorithm(), new TermExFeatureWrapper(termDocFreq, wordFreq, bncRef));
                tester.registerAlgorithm(new RIDFAlgorithm(), new RIDFFeatureWrapper(termCorpusFreq));
                tester.registerAlgorithm(new AverageCorpusTFAlgorithm(), new AverageCorpusTFFeatureWrapper(termCorpusFreq));
                tester.registerAlgorithm(new FrequencyAlgorithm(), new FrequencyFeatureWrapper(termCorpusFreq));

				tester.execute(termDocIndex, args[2]);
				System.out.println(new Date());

			}
			catch (Exception e) {
				e.printStackTrace();
			}
		}
	}
}}}



= Implementing new algorithms =